{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os, shutil, pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import image_dataset_from_directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:16:06.046295Z",
     "start_time": "2025-04-01T15:15:55.683961Z"
    }
   },
   "id": "935d0c9baac3870d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resize and rename images\n",
    "\n",
    "```python\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_and_rename_images(input_folder, output_folder, name, new_width=224, new_height=224):\n",
    "    prefix = name\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    files = [f for f in os.listdir(input_folder) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    for index, file in enumerate(files, start=1):\n",
    "        img_path = os.path.join(input_folder, file)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "                new_filename = f\"{prefix}.{index:02d}.jpg\"\n",
    "                new_path = os.path.join(output_folder, new_filename)\n",
    "                img.save(new_path, \"JPEG\")\n",
    "                print(f\"Processed and saved: {new_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "\n",
    "names = [\"haarukka\", \"lusikka\", \"veitsi\"]\n",
    "for name in names:\n",
    "    resize_and_rename_images(f\"aterimet/{name}\", f\"aterimet_small\", name)\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ee7cfc6ec12842d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Directories for Subsets\n",
    "- Defining the base directory for output and the directory containing the original images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9dd86d97dae4e90"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_base_dir = \"kuvat\"  # assuming base directory for output\n",
    "original_dir = \"aterimet_small\"  # assuming base directory for original images"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T15:37:56.994774Z",
     "start_time": "2025-03-31T15:37:56.967879Z"
    }
   },
   "id": "b3b25aab5c5ccd91",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to Create Subsets\n",
    "-  A function to create subsets of the data for training, validation, and testing.\n",
    "-  Create subsets: training (1-41), validation (42-50), and test (51-59) data\n",
    "\n",
    "```python\n",
    "\n",
    "# Function to create subsets\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"haarukka\", \"lusikka\", \"veitsi\"):\n",
    "        dir = os.path.join(new_base_dir, subset_name, category)\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        fnames = [f\"{category}.{i:02d}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=os.path.join(original_dir, fname), dst=os.path.join(dir, fname))\n",
    "            \n",
    "make_subset(\"train\", start_index=1, end_index=43)\n",
    "make_subset(\"validation\", start_index=43, end_index=52)\n",
    "make_subset(\"test\", start_index=52, end_index=61)\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e80af2b534893f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Datasets\n",
    "- Use TensorFlow's `image_dataset_from_directory` to load the datasets for training, validation, and testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b00cbb2fb3dc2d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 files belonging to 3 classes.\n",
      "Found 27 files belonging to 3 classes.\n",
      "Found 27 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    os.path.join(new_base_dir, \"train\"),\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    os.path.join(new_base_dir, \"validation\"),\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    os.path.join(new_base_dir, \"test\"),\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T15:37:57.199242Z",
     "start_time": "2025-03-31T15:37:57.001536Z"
    }
   },
   "id": "70ac5010fbe1207e",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Augmentation\n",
    "Apply augmentation only to the training dataset.\n",
    "- Random Horizontal Flip: Randomly flips the image left to right.\n",
    "\n",
    "- Random Rotation: Rotates the image by up to 36 degrees (both clockwise and counterclockwise).\n",
    "\n",
    "- Random Zoom: Zooms in on the image by up to 20%.\n",
    "\n",
    "These transformations help the model become more flexible and better at recognizing objects in different orientations, sizes, and positions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d884b1ecd68a11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply augmentation only to the training dataset.\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x), y))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T15:37:57.336729Z",
     "start_time": "2025-03-31T15:37:57.179985Z"
    }
   },
   "id": "784ed3f471bc1417",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization\n",
    "- Normalize the images by rescaling pixel values to the range [0, 1]. This is done by dividing the pixel values by 255. \n",
    "- scale = 1/255 â†’ Divides all pixel values by 255"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "847da94aeaf17d96"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "# Normalize pixel values to the range [0,1] by dividing by 255\n",
    "normalization_layer = Rescaling(1./255)\n",
    "\n",
    "# Apply normalization to the datasets\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T15:37:57.374144Z",
     "start_time": "2025-03-31T15:37:57.338973Z"
    }
   },
   "id": "4abc54abfe31ec12",
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
